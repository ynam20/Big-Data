{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `caobd_s19` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Wednesday*, February 13, 2019, 23:59<br>\n",
    "**Peergrading deadline**: *Sunday*, February 16, 2019, 23:59<br>\n",
    "**Peergrading feedback deadline**: *Wednesday*, February 20, 2019, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 1.1.10**: `dict`s and `defaultdict`s.\n",
    "1. What is a `defaultdict`? How would you say it is different from a normal Python `dict`?\n",
    "2. Write some code that takes a list of tuples:\n",
    "\n",
    ">        l = [(\"a\", 1), (\"b\", 3), (\"a\", None), (\"c\", False), (\"b\", True), (\"a\", None)]\n",
    "\n",
    ">     And produces a `defaultdict` object\n",
    "\n",
    ">        defaultdict(<type 'list'>, {'a': [1, None, None], 'c': [False], 'b': [3, True]})\n",
    "\n",
    ">*Hint: you can import `defaultdict` from `collections`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 1.2.4**: The URL reveals that the data is from reddit/r/gameofthrones, but can you recover that information from the data? Give your answer by 'keying' into the JSON data using square brackets.\n",
    "\n",
    ">*Hint: 'Keying' is a word i just made up. By it, I mean the following. Consider a JSON object such as:*\n",
    ">\n",
    ">        my_json_obj = {\n",
    ">            'cats': {\n",
    ">                'awesome': ['Missy'],\n",
    ">                'useless': ['Kim', 'Frank', 'Sandy']\n",
    ">            },\n",
    ">            'dogs': {\n",
    ">                'awesome': ['Finn', 'Dolores', 'Fido', 'Casper'],\n",
    ">                'useless': []\n",
    ">            }\n",
    ">        }\n",
    ">\n",
    ">*I can get the list of useless cats by keying into `my_json_obj` like such:*\n",
    ">\n",
    ">        >>> my_json_obj['cats']['useless']\n",
    ">        Out [ ]: ['Kim', 'Frank', 'Sandy']\n",
    ">\n",
    ">*`my_json_obj['cats']` returns the dictionary `{'awesome': ['Missy'], 'useless': ['Kim', 'Frank', 'Sandy']}` and getting '`useless`' from that eventually gives us `['Kim', 'Frank', 'Sandy']`. If any of those list items were a list of a dictionary themselves, we could have kept keying deeper into the structure.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 1.2.5**: Write two `for` loops (or list comprehensions for extra street credits) which:\n",
    ">1. Counts the number of spoilers.\n",
    ">2. Only prints headlines that aren't spoilers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.2**: The `get_x_y` function below gives you the number of comments versus score for the latest `N` posts on a given `subreddit`.\n",
    "3. In two seperate figures, floating side by side, scatter plot (left) the set of x and y variables for \"blackmirror\" and (right) x and y for \"news\". Remember to transform the data. My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2b.png).\n",
    "4. Comment on any differences you see in the trends. Why might number of comments versus post upvotes look different for a TV-show than for world news?\n",
    ">\n",
    ">*Hint: By \"transformation\" I explicitly mean that you map some function onto every value in a list of values. Example: I can apply a square root transformation like `x = [np.sqrt(v) for v in x]`. A faster way to do that, of course, would be just `x = np.sqrt(x)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T21:28:54.941490Z",
     "start_time": "2019-01-23T21:28:44.166852Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "\n",
    "def get_x_y(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            x.append(d['data']['num_comments'])\n",
    "            y.append(d['data']['score'])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return x, y\n",
    "                          \n",
    "x, y = get_x_y(\"news\", 500, count=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.4**: In Data Science, we often think of matrices as (usually two-dimensional) containers for data. Let's say we collect $N=500$ data points, that each has $M=10$ features. We can loslessly represent that data using an $N \\times M$ matrix, that is a matrix that has a row for each datapoint and a column for each feature. In fact, let's just go ahead and do that by altering the code of the `get_x_y` function from before a little bit.\n",
    ">\n",
    ">*Note: `numpy` has an object type called `matrix` but we rarely use that. Instead, we represent matrices as a `numpy` object type called `array`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T07:49:55.947679Z",
     "start_time": "2019-01-18T07:49:46.008308Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_matrix(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    X = []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            X.append([\n",
    "                int(d['data']['num_comments']),\n",
    "                int(d['data']['score']),\n",
    "                int(d['data']['ups']),\n",
    "                int(d['data']['downs']),\n",
    "                len(d['data']['selftext']),\n",
    "                len(d['data']['title']),\n",
    "                int(d['data']['is_original_content']),\n",
    "                int(d['data']['spoiler']),\n",
    "                int(d['data']['num_crossposts']),\n",
    "                int(d['data']['is_video'])\n",
    "            ])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return np.array(X)\n",
    "                          \n",
    "X = get_data_matrix(\"news\", 500, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T07:49:57.572095Z",
     "start_time": "2019-01-18T07:49:57.567762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2131,  8709,  8709, ...,     0,     3,     0],\n",
       "       [  565, 21698, 21698, ...,     0,     8,     0],\n",
       "       [  325,  3789,  3789, ...,     0,     2,     0],\n",
       "       ...,\n",
       "       [    3,    10,    10, ...,     0,     0,     0],\n",
       "       [   28,    44,    44, ...,     0,     0,     0],\n",
       "       [    7,    41,    41, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Here are the data, but how can we summarize them? Let's start by finding the so-called *covariance matrix*.\n",
    "1. Use the `np.cov` method on `X` to get its $10 \\times 10$ covariance matrix.\n",
    "2. Do you notice any characteristics of this matrix that are interesting? Comment.\n",
    "3. Plot the distribution of covariances, e.g. using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.5**: There's another use of the covariance matrix, other than just learning how features co-vary. In fact, it turns out that the *eigenvectors* of the covariance matrix are a set of mutually orthogonal vectors, that point in the directions of greatest variance in the data. The eigenvector with the greatest *eigenvalue* points along the direction of greatest variation, and so on. This is pretty neat, because if we know along which axes the data is most stretched, we can figure out how best to project it when visualizing it in 2D as a scatter plot! This whole procedure has a name: **Principal Component Analysis** (PCA) and it was invented by Karl Pearson in 1901. It belongs to a powerful class of linear algebra methods called **Matrix Factorization** methods. Ok, so rather than spending too much time on the math of PCA, let's just use the `sklearn` implementation and fit a PCA on `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T08:30:52.583430Z",
     "start_time": "2019-01-18T08:30:52.577358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. Explain what the matrix you get when you call `pca.components_` means.\n",
    "2. Explain what the vector you get when you call `pca.explained_variance_ratio_` means. What insights about our data can we extract from this?\n",
    "3. Transform X using the PCA you just fitted, and scatter plot the first two dimensions of the transformed data. Please comment on what you see.\n",
    "3. Scatter plot dimensions 1 and 3 against each other. See something interesting now? What would be a way to figure out what the clusters represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.4**: People often use the p-value to gauge the *significance* of a given result. If the p-value of a result is low, the result is significant (which is good) and vice versa. Intuitively, the p-value measures the probabilty that a result *could have been obtained at random*, so you can imagine that if you find that the p-value for some result is HIGH (close to one), regardless of how cool it is, people will not care because, well, you just got lucky with that measurement, didn't you? I created two lists for you below, and you are going to find out if they are *significantly* correlated. You will be using the significance threshold 0.05 (which is arbitrary, disputed, yet very standard in the literature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T09:50:21.670665Z",
     "start_time": "2019-01-24T09:50:21.666647Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can incresae this to make the data more noisy (but let it be 4 for now)\n",
    "noise_level = 4\n",
    "\n",
    "# I'm just seeding the random number generator here, so we can compare results\n",
    "np.random.seed(0)\n",
    "\n",
    "# This is your data\n",
    "x = np.arange(0, 20) + np.random.normal(size=20) * noise_level\n",
    "y = np.arange(0, 20) + np.random.normal(size=20) * noise_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T09:55:18.924607Z",
     "start_time": "2019-01-24T09:55:18.922022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Why not make a scatter plot here, to see what you're working with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">4. Plot the p-value as a function of `noise_level` (let `noise_level` vary between 1 and 50), so we can see how our result becomes less and less significant as we increase the noise. Two questions: (1) at which value of `noise_level` does the correlation become insignificant, and (2) which p-value does the curve saturate at for large `noise_level`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 3.1.1**: From the Wikipedia API, get a list of all Marvel superheroes and another list of all Marvel supervillains. Use 'Category:Marvel_Comics_supervillains' and 'Category:Marvel_Comics_superheroes' to get the characters in each category.\n",
    "1. How many superheroes are there? How many supervillains?\n",
    "2. How many characters are both heroes and villains? What is the Jaccard similarity between the two groups?\n",
    "\n",
    ">*Hint: Google something like \"get list all pages in category wikimedia api\" if you're struggling with the query.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.1**: Extract the length of the page of each character, and plot the distribution of this variable for each class (heroes/villains/ambiguous). Can you say anything about the popularity of characters in the Marvel universe based on your visualization?\n",
    "\n",
    ">*Hint: The simplest thing is to make a probability mass function, i.e. a normalized histogram. Use `plt.hist` on a list of page lengths, with the argument `density=True`. Other distribution plots are fine too, though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.2**: Find the 10 characters from each class with the longest Wikipedia pages. Visualize their page lengths with bar charts. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.3**: We are interested in knowing if there is a time-trend in the debut of characters.\n",
    "* Extract into three lists, debut years of heroes, villains, and ambiguous characters.\n",
    "* Do all pages have a debut year? Do some have multiple? How do you handle these inconsistencies?\n",
    "* For each class, visualize the amount of characters introduced over time. You choose how you want to visualize this data, but please comment on your choice. Also comment on the outcome of your analysis.\n",
    "\n",
    ">*Hint: The debut year is given on the debut row in the info table of a character's Wiki-page. There are many ways that you can extract this variable. You should try to have a go at it yourself, but if you are short on time, you can use this horribly ugly regular expression code:*\n",
    "\n",
    ">*`re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup_text)[0])[0][:-1]`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
